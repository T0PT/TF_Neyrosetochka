{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_floats(string_list):\n",
    "    letters = ' abcdefghijklmnopqrstuvwxyz,.-()/\"'+\"'0123456789\"\n",
    "    # Get the number of possible letter # add 1 for the ' ' character\n",
    "    # Initialize an empty list to store the results\n",
    "    result = []\n",
    "\n",
    "    # Loop over each string in the input list\n",
    "    for string in string_list:\n",
    "        strng=str(string).lower()\n",
    "        strng+=' '*25\n",
    "        to_add=[]\n",
    "        # for v in range(25):\n",
    "            # if strng[v] in [',',',','.','-','(',')','/','\"',\"'\"]:\n",
    "            #     strng=strng[:v]+strng[v+1:]\n",
    "            #     left+=1\n",
    "        for v in range(25):\n",
    "            to_add.append((letters.index(strng[v]))/len(letters))\n",
    "        result.append(to_add)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1  2  3  7  8  9 10]\n",
      " [ 4  5  6 11 12 13 14]], shape=(2, 7), dtype=int32)\n",
      "PassengerId   int64\n",
      "Survived   int64\n",
      "Pclass   int64\n",
      "Name   object\n",
      "Sex   object\n",
      "Age   float64\n",
      "SibSp   int64\n",
      "Parch   int64\n",
      "Ticket   object\n",
      "Fare   float64\n",
      "Cabin   object\n",
      "Embarked   object\n",
      "dict_keys(['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'])\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ConcatV2_N_10_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [891] vs. shape[1] = [891,25] [Op:ConcatV2] name: flattened_list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m features\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m     33\u001b[0m     to_feed\u001b[39m.\u001b[39mappend(tf\u001b[39m.\u001b[39mconstant(features[key]))\n\u001b[1;32m---> 34\u001b[0m to_feed\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39;49mconcat(to_feed, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mflattened_list\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7214\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7215\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ConcatV2_N_10_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [891] vs. shape[1] = [891,25] [Op:ConcatV2] name: flattened_list"
     ]
    }
   ],
   "source": [
    "t1 = [[1, 2, 3], [4, 5, 6]]\n",
    "t2 = [[7, 8, 9,10], [11, 12, 13, 14]]\n",
    "print(tf.concat([t1,t2], axis=1))\n",
    "\n",
    "features={}\n",
    "answrs=np.array([])\n",
    "for key in df.keys():\n",
    "    print(str(key) +'   '+str(df[key].dtype))\n",
    "    if key!= 'Survived':\n",
    "        if df[key].dtype=='int64' and key!= 'PassengerId':  #ints to float32\n",
    "            features[key]=np.array(df[key] - df[key].min()) / (df[key].max() - df[key].min())\n",
    "\n",
    "        elif df[key].dtype=='object' and key in ['Sex', 'Embarked']:  #Sex and Embarked to float32\n",
    "            to_add=np.array([])\n",
    "            for one in df[key]:\n",
    "                if one=='male':\n",
    "                    to_add=np.append(to_add, 1)\n",
    "                else:\n",
    "                    to_add=np.append(to_add, 0)\n",
    "            features[key]=to_add\n",
    "\n",
    "        elif df[key].dtype=='object' and key in ['Name', 'Ticket', 'Cabin']:  #['Name', 'Ticket', 'Cabin'] to float32\n",
    "            features[key]=tf.convert_to_tensor(string_to_floats(df[key]))\n",
    "\n",
    "        elif df[key].dtype=='float64': #floats to float32\n",
    "            features[key]=np.array(df[key] - df[key].min()) / (df[key].max() - df[key].min())\n",
    "\n",
    "    else:\n",
    "        answrs=np.append(answrs, df[key])\n",
    "print(features.keys())\n",
    "to_feed=[]\n",
    "for key in features.keys():\n",
    "    to_feed.append(tf.constant(features[key]))\n",
    "to_feed=tf.concat(to_feed, axis=0, name='flattened_list')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexb\\AppData\\Local\\Temp\\ipykernel_11952\\1426083204.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  model.fit(np.array(to_feed).T, answrs, epochs=10, batch_size=2)#, validation_data=(x_val, y_val))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (891,25) into shape (891,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential([\n\u001b[0;32m      2\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m16\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, input_dim\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m),\n\u001b[0;32m      3\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m1\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     ])\n\u001b[0;32m      6\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m model\u001b[39m.\u001b[39mfit(np\u001b[39m.\u001b[39;49marray(to_feed)\u001b[39m.\u001b[39mT, answrs, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m#, validation_data=(x_val, y_val))\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (891,25) into shape (891,)"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_dim=7),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(np.array(to_feed).T, answrs, epochs=10, batch_size=2)#, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Titanic\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Titanic\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Titanic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fd2a919270971e76d246c0ffd5a089272b911b92146131cce460b1d8c4074d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
